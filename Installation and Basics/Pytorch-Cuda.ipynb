{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Support For CUDA\n",
    "\n",
    "1) Pytorch Tensor have been architected to make optimal use of GPUs for massively parallel computing<BR>\n",
    "2) Pytorch enable developer to create tensors on cuda device which get's performance benifits straight away.<BR>\n",
    "3) By default all operations on GPU is asynchonous, but we can force to execute synchonous using envitonmental variable CUDA_LAUNCH_BLOCKING = 1 mainly useful for error handling examining stack traces<br>\n",
    "4) Pytorch also helps to writa a single code which works on both CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Api \n",
    "\n",
    "##### Install pytorch with cuda support\n",
    "\n",
    "Run this command - Make sure you install latest driver from Nvdia\n",
    "<b>conda install pytorch torchvision cudatoolkit=10.2 -c pytorch</b> in Anaconda Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is available? True\n",
      "Device Count? 1\n",
      "Memory Allocated? 0\n",
      "Memory Cached? 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Is available?\" , torch.cuda.is_available())\n",
    "print(\"Device Count?\" , torch.cuda.device_count())\n",
    "print(\"Memory Allocated?\" , torch.cuda.memory_allocated())\n",
    "print(\"Memory Cached?\" , torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get reference of CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "cuda0 = torch.device('cuda:0')\n",
    "print(cuda, cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tensor in CUDA device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "arr = torch.tensor([100., 200.,300.,400.])\n",
    "print(arr.device)\n",
    "arr = torch.tensor([100., 200.,300.,400.], device =cuda)\n",
    "print(arr.device)\n",
    "arr = torch.tensor([100., 200.,300.,400.], device =cuda0)\n",
    "print(arr.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a tensor from CPU to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "arr = torch.tensor([100., 200.,300.,400.])\n",
    "print(arr.device)\n",
    "arrCuda = arr.cuda()\n",
    "print(arrCuda.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device ContextManager\n",
    "\n",
    "Helps to switch between the CUDA devices. Hence my Laptop has only one CUDA device will not able to show the output but the code will be something like below. The \"WITH\" keyword below helps to switch device context\n",
    "\n",
    "with torch.cuda.device(1):    \n",
    "    arr = torch.tensor([100., 200.,300.,400.])    \n",
    "    arr1 = torch.tensor([100., 200.,300.,400.], device=cuda0)\n",
    "    \n",
    "\n",
    "Note: You can't perform operation across different CUDA devices(means for ex1: cannot perform any operation between tensor1 in cuda0 and tensor2 in cuda1)\n",
    "\n",
    "To do operation , we need to copy the tensor from the device(in above case, copy tensor1 to device cuda1)\n",
    "\n",
    "arr0 = arr1.to(device=cuda1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is available? True\n",
      "Device Count? 1\n",
      "Memory Allocated? 512\n",
      "Memory Cached? 2097152\n"
     ]
    }
   ],
   "source": [
    "print(\"Is available?\" , torch.cuda.is_available())\n",
    "print(\"Device Count?\" , torch.cuda.device_count())\n",
    "print(\"Memory Allocated?\" , torch.cuda.memory_allocated())\n",
    "print(\"Memory Cached?\" , torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's clear the memory - useful step to clear the unused memory.\n",
    "\n",
    "Since we don't have unused memory and the memory cache size holds the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Cached? 2097152\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Memory Cached?\" , torch.cuda.memory_cached())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
